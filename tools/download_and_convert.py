#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‹è½½HuggingFaceæ¨¡å‹å¹¶è½¬æ¢ä¸ºOllamaå…¼å®¹æ ¼å¼çš„è„šæœ¬
"""

import os
import sys
import subprocess
import tempfile
import shutil
from pathlib import Path

def run_command(cmd, description="", cwd=None):
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    print(f"ğŸ”„ {description}")
    print(f"   Command: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=cwd)
        if result.returncode == 0:
            print(f"âœ… {description} æˆåŠŸ!")
            if result.stdout.strip():
                print(f"   è¾“å‡º: {result.stdout.strip()}")
            return True, result.stdout
        else:
            print(f"âŒ {description} å¤±è´¥!")
            if result.stderr.strip():
                print(f"   é”™è¯¯: {result.stderr.strip()}")
            return False, result.stderr
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå‘½ä»¤æ—¶å‡ºé”™: {e}")
        return False, str(e)

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")
    
    # æ£€æŸ¥git-lfs
    success, _ = run_command("git lfs version", "æ£€æŸ¥git-lfs")
    if not success:
        print("âŒ git-lfsæœªå®‰è£…!")
        print("ğŸ’¡ å®‰è£…æ–¹æ³•: brew install git-lfs")
        return False
    
    # æ£€æŸ¥huggingface-cli
    success, _ = run_command("pip show huggingface_hub", "æ£€æŸ¥huggingface_hub")
    if not success:
        print("âš ï¸ huggingface_hubæœªå®‰è£…ï¼Œå°è¯•å®‰è£…...")
        success, _ = run_command("pip install --user huggingface_hub", "å®‰è£…huggingface_hub")
        if not success:
            print("âŒ æ— æ³•å®‰è£…huggingface_hub")
            return False
    
    return True

def download_model_from_hf(model_name, local_path):
    """ä»HuggingFaceä¸‹è½½æ¨¡å‹"""
    print(f"\nğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name}")
    print(f"   ç›®æ ‡è·¯å¾„: {local_path}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    Path(local_path).mkdir(parents=True, exist_ok=True)
    
    # ä½¿ç”¨git cloneä¸‹è½½
    repo_url = f"https://huggingface.co/{model_name}"
    cmd = f"git clone {repo_url} {local_path}"
    
    success, output = run_command(cmd, f"å…‹éš†æ¨¡å‹ä»“åº“ {model_name}")
    
    if success:
        # ä¸‹è½½LFSæ–‡ä»¶
        success, _ = run_command("git lfs pull", f"ä¸‹è½½å¤§æ–‡ä»¶", cwd=local_path)
    
    return success

def create_ollama_modelfile(model_path, model_name):
    """åˆ›å»ºOllama Modelfile"""
    modelfile_content = f"""FROM {model_path}

TEMPLATE \"\"\"<|im_start|>system
You are a helpful assistant that generates high-quality mathematical problems. Please generate clear, well-structured problems with appropriate difficulty levels.
<|im_end|>
<|im_start|>user
{{{{ .Prompt }}}}
<|im_end|>
<|im_start|>assistant
\"\"\"

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1

SYSTEM \"\"\"You are an expert mathematics problem generator. Generate educational and challenging mathematical problems across various topics including algebra, geometry, calculus, and statistics.\"\"\"
"""
    
    filename = f"{model_name}.modelfile"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    print(f"ğŸ“ åˆ›å»ºModelfile: {filename}")
    return filename

def import_to_ollama(model_path, ollama_name):
    """å¯¼å…¥æ¨¡å‹åˆ°Ollama"""
    print(f"\nğŸ¦™ å¯¼å…¥åˆ°Ollama: {ollama_name}")
    
    # åˆ›å»ºModelfile
    modelfile = create_ollama_modelfile(model_path, ollama_name)
    
    try:
        # åˆ›å»ºOllamaæ¨¡å‹
        cmd = f"ollama create {ollama_name} -f {modelfile}"
        success, output = run_command(cmd, f"åˆ›å»ºOllamaæ¨¡å‹ {ollama_name}")
        
        if success:
            print(f"âœ… æˆåŠŸåˆ›å»ºOllamaæ¨¡å‹: {ollama_name}")
            print(f"ğŸ’¡ ä½¿ç”¨æ–¹æ³•: ollama run {ollama_name}")
            return True
        else:
            print(f"âŒ åˆ›å»ºOllamaæ¨¡å‹å¤±è´¥")
            return False
            
    finally:
        # æ¸…ç†Modelfile
        if os.path.exists(modelfile):
            os.remove(modelfile)
            print(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {modelfile}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ HuggingFace -> Ollama æ¨¡å‹å¯¼å…¥å·¥å…·")
    print("="*60)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥!")
        return
    
    # ä½ çš„æ¨¡å‹åˆ—è¡¨
    models = {
        "final": "xingqiang/questions-gen-qwen3-14b-final-merged-16bit",
        "stage2": "xingqiang/questions-gen-qwen3-14b-stage2-merged-16bit", 
        "stage1": "xingqiang/QuestionsGen-Qwen3-14b-stage1-fp-merged"
    }
    
    print(f"\nğŸ“‹ å¯ç”¨æ¨¡å‹:")
    for i, (stage, model_name) in enumerate(models.items(), 1):
        print(f"   {i}. {stage}: {model_name}")
    
    # é€‰æ‹©æ¨¡å‹
    choice = input(f"\né€‰æ‹©è¦å¯¼å…¥çš„æ¨¡å‹:\n1. finalæ¨¡å‹ (æ¨è)\n2. æ‰€æœ‰æ¨¡å‹\n3. è‡ªå®šä¹‰é€‰æ‹©\nè¯·è¾“å…¥ (1/2/3): ").strip()
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp(prefix="ollama_import_")
    print(f"ğŸ“ ä¸´æ—¶ç›®å½•: {temp_dir}")
    
    try:
        if choice == "1":
            # åªä¸‹è½½finalæ¨¡å‹
            model_name = models["final"]
            local_path = os.path.join(temp_dir, "final")
            
            if download_model_from_hf(model_name, local_path):
                import_to_ollama(local_path, "questions-gen")
                
        elif choice == "2":
            # ä¸‹è½½æ‰€æœ‰æ¨¡å‹
            for stage, model_name in models.items():
                local_path = os.path.join(temp_dir, stage)
                
                if download_model_from_hf(model_name, local_path):
                    ollama_name = f"questions-gen-{stage}"
                    import_to_ollama(local_path, ollama_name)
                else:
                    print(f"âŒ è·³è¿‡ {stage} æ¨¡å‹")
                    
        elif choice == "3":
            # è‡ªå®šä¹‰é€‰æ‹©
            for stage, model_name in models.items():
                if input(f"ä¸‹è½½ {stage} æ¨¡å‹? (y/N): ").lower().startswith('y'):
                    local_path = os.path.join(temp_dir, stage)
                    
                    if download_model_from_hf(model_name, local_path):
                        ollama_name = f"questions-gen-{stage}"
                        import_to_ollama(local_path, ollama_name)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©!")
            return
            
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        print(f"\nğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“‹ æŸ¥çœ‹å¯¼å…¥çš„æ¨¡å‹:")
    run_command("ollama list | grep questions-gen", "æŸ¥çœ‹Questions-Genæ¨¡å‹")
    
    print(f"\nğŸ‰ å¯¼å…¥å®Œæˆ!")
    print(f"ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print(f"   ollama run questions-gen 'Generate a calculus problem'")
    print(f"   ollama run questions-gen-final 'Create a geometry problem'")

if __name__ == "__main__":
    main()
