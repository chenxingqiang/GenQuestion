#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›´æ¥ä»HuggingFaceå¯¼å…¥æ¨¡å‹åˆ°Ollamaçš„è„šæœ¬
"""

import subprocess
import sys
import os

def run_command(cmd, description=""):
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    print(f"ğŸ”„ {description}")
    print(f"   Command: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… {description} æˆåŠŸ!")
            if result.stdout.strip():
                print(f"   è¾“å‡º: {result.stdout.strip()}")
            return True
        else:
            print(f"âŒ {description} å¤±è´¥!")
            if result.stderr.strip():
                print(f"   é”™è¯¯: {result.stderr.strip()}")
            return False
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå‘½ä»¤æ—¶å‡ºé”™: {e}")
        return False

def create_modelfile(hf_model_name, model_name):
    """åˆ›å»ºModelfile"""
    modelfile_content = f"""FROM {hf_model_name}

TEMPLATE \"\"\"<|im_start|>system
You are a helpful assistant that generates high-quality mathematical problems.
<|im_end|>
<|im_start|>user
{{{{ .Prompt }}}}
<|im_end|>
<|im_start|>assistant
\"\"\"

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
"""
    
    filename = f"{model_name}.modelfile"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    print(f"ğŸ“ åˆ›å»ºModelfile: {filename}")
    return filename

def import_model_to_ollama(hf_model_name, local_name):
    """å°†HuggingFaceæ¨¡å‹å¯¼å…¥åˆ°Ollama"""
    print(f"\nğŸš€ å¯¼å…¥æ¨¡å‹: {hf_model_name} -> {local_name}")
    print("="*60)
    
    # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨ollama pullï¼ˆå¦‚æœæ¨¡å‹åœ¨Ollama Hubä¸­ï¼‰
    print(f"ğŸ“¥ å°è¯•ç›´æ¥æ‹‰å–æ¨¡å‹...")
    cmd = f"ollama pull {hf_model_name}"
    if run_command(cmd, f"ç›´æ¥æ‹‰å– {hf_model_name}"):
        # é‡å‘½åæ¨¡å‹
        cmd = f"ollama cp {hf_model_name} {local_name}"
        return run_command(cmd, f"é‡å‘½åä¸º {local_name}")
    
    # æ–¹æ³•2: ä½¿ç”¨Modelfileåˆ›å»º
    print(f"ğŸ“ ä½¿ç”¨Modelfileæ–¹å¼...")
    modelfile = create_modelfile(hf_model_name, local_name)
    
    cmd = f"ollama create {local_name} -f {modelfile}"
    success = run_command(cmd, f"åˆ›å»ºæ¨¡å‹ {local_name}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if os.path.exists(modelfile):
        os.remove(modelfile)
        print(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {modelfile}")
    
    return success

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¦™ Questions-Gen HuggingFace -> Ollama å¯¼å…¥å·¥å…·")
    print("="*60)
    
    # æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨
    if not run_command("ollama --version", "æ£€æŸ¥Ollamaç‰ˆæœ¬"):
        print("âŒ Ollamaæœªå®‰è£…æˆ–ä¸å¯ç”¨!")
        return
    
    # ä½ çš„ä¸‰ä¸ªæ¨¡å‹
    models = {
        "stage1": "xingqiang/QuestionsGen-Qwen3-14b-stage1-fp-merged",
        "stage2": "xingqiang/questions-gen-qwen3-14b-stage2-merged-16bit", 
        "final": "xingqiang/questions-gen-qwen3-14b-final-merged-16bit"
    }
    
    print(f"\nğŸ“‹ å‘ç° {len(models)} ä¸ªæ¨¡å‹å¾…å¯¼å…¥:")
    for stage, hf_name in models.items():
        print(f"   {stage}: {hf_name}")
    
    # è¯¢é—®ç”¨æˆ·
    choice = input(f"\né€‰æ‹©å¯¼å…¥æ¨¡å¼:\n1. å¯¼å…¥æ‰€æœ‰æ¨¡å‹\n2. å¯¼å…¥finalæ¨¡å‹ï¼ˆæ¨èï¼‰\n3. è‡ªå®šä¹‰é€‰æ‹©\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
    
    if choice == "1":
        # å¯¼å…¥æ‰€æœ‰æ¨¡å‹
        print(f"\nğŸš€ å¯¼å…¥æ‰€æœ‰æ¨¡å‹...")
        success_count = 0
        for stage, hf_name in models.items():
            local_name = f"questions-gen-{stage}"
            if import_model_to_ollama(hf_name, local_name):
                success_count += 1
        
        print(f"\nğŸ“Š å¯¼å…¥ç»“æœ: {success_count}/{len(models)} æˆåŠŸ")
        
    elif choice == "2":
        # åªå¯¼å…¥finalæ¨¡å‹
        print(f"\nğŸš€ å¯¼å…¥finalæ¨¡å‹...")
        hf_name = models["final"]
        local_name = "questions-gen"
        success = import_model_to_ollama(hf_name, local_name)
        if success:
            print(f"âœ… æˆåŠŸå¯¼å…¥! ä½¿ç”¨æ–¹æ³•: ollama run questions-gen")
        
    elif choice == "3":
        # è‡ªå®šä¹‰é€‰æ‹©
        print(f"\nğŸ”§ è‡ªå®šä¹‰æ¨¡å¼:")
        for stage, hf_name in models.items():
            if input(f"å¯¼å…¥ {stage} æ¨¡å‹? (y/N): ").lower().startswith('y'):
                local_name = f"questions-gen-{stage}"
                import_model_to_ollama(hf_name, local_name)
    
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©!")
        return
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    print(f"\nğŸ“‹ æŸ¥çœ‹å·²å¯¼å…¥çš„æ¨¡å‹:")
    run_command("ollama list", "åˆ—å‡ºæ‰€æœ‰æ¨¡å‹")
    
    print(f"\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print("   ollama run questions-gen 'Generate a calculus problem:'")
    print("   ollama run questions-gen-final 'Create a geometry problem:'")

if __name__ == "__main__":
    main()
