# Questions-Gen è®­ç»ƒæµç¨‹è¯¦ç»†æŠ€æœ¯æ–‡æ¡£

## ğŸ¯ æ€»ä½“æ¶æ„

åŸºäºQwen3-14Bçš„ä¸‰é˜¶æ®µé€’è¿›å¼è®­ç»ƒpipelineï¼Œä¸“é—¨ç”¨äºç”Ÿæˆé«˜è´¨é‡æ•°å­¦ç«èµ›é¢˜ç›®ã€‚

## ğŸš€ **é˜¶æ®µä¸€ï¼šåŸºç¡€é¢„è®­ç»ƒ (Stage 1: Basic Pretraining)**

### ç›®æ ‡
å»ºç«‹åŸºç¡€çš„æ•°å­¦é—®é¢˜ç”Ÿæˆèƒ½åŠ›

### æ•°æ®å‡†å¤‡
1. **çœŸå®æ•°æ®é›†åŠ è½½**ï¼š
   - ä½¿ç”¨`OpenMathReasoning-mini`æ•°æ®é›†ï¼ˆ19,252æ¡æ•°å­¦æ¨ç†æ•°æ®ï¼‰
   - ä½¿ç”¨`FineTome-100k`å¯¹è¯æ•°æ®é›†ï¼ˆ100,000æ¡å¯¹è¯æ•°æ®ï¼‰
   - æŒ‰25%æ¯”ä¾‹æ··åˆæ¨ç†å’Œå¯¹è¯æ•°æ®ï¼ˆ25,669æ¡æ€»æ•°æ®ï¼‰

2. **æ•°æ®æ ¼å¼è½¬æ¢**ï¼š
   ```python
   # è½¬æ¢ä¸ºunslothæ ‡å‡†æ ¼å¼
   conversations = [
       [
           {"role": "user", "content": "Generate a challenging algebra problem:"},
           {"role": "assistant", "content": "Find x: x^4 - 5x^2 + 6 = 0..."}
       ]
   ]
   ```

### æ¨¡å‹é…ç½®
```python
# æ¨¡å‹åŠ è½½ï¼ˆè§£å†³äº†æ‰€æœ‰å…¼å®¹æ€§é—®é¢˜ï¼‰
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen3-14B",
    max_seq_length=2048,
    load_in_4bit=True,
    trust_remote_code=True,  # è§£å†³attention biasé”™è¯¯
)

# LoRAé…ç½®
model = FastLanguageModel.get_peft_model(
    model,
    r=32,                    # LoRA rank
    lora_alpha=32,          # LoRA alpha
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                    "gate_proj", "up_proj", "down_proj"]
)
```

### è®­ç»ƒå‚æ•°
```python
SFTConfig(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,    # æœ‰æ•ˆbatch size = 8
    max_steps=100,                    # è®­ç»ƒæ­¥æ•°
    learning_rate=2e-4,               # å­¦ä¹ ç‡
    fp16=False, bf16=True,           # æ­£ç¡®çš„ç²¾åº¦è®¾ç½®
    optim="adamw_8bit",              # 8bitä¼˜åŒ–å™¨
)
```

## ğŸ¯ **é˜¶æ®µäºŒï¼šGRPOå¼ºåŒ–å­¦ä¹ ä¼˜åŒ– (Stage 2: RL GRPO Training)**

### ç›®æ ‡
é€šè¿‡ç¾¤ä½“å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æå‡é—®é¢˜è´¨é‡ï¼Œå¹¶è®­ç»ƒå˜ä½“ç”Ÿæˆèƒ½åŠ›

### æ ¸å¿ƒæµç¨‹ï¼ˆæ¯ä¸ªè®­ç»ƒæ­¥éª¤ï¼‰

#### 1. **åŸå§‹é—®é¢˜ç”Ÿæˆè®­ç»ƒ**
```python
# ç”Ÿæˆé—®é¢˜ç»„ï¼ˆ8ä¸ªé—®é¢˜ï¼‰
group_questions = self._generate_question_group()

# å¤šç»´åº¦å¥–åŠ±è®¡ç®—
for question in group_questions:
    reward = reward_calculator.calculate_reward(
        question, history_questions, group_questions
    )
    # å¥–åŠ±ç»´åº¦ï¼šéš¾åº¦(40%) + æ–°é¢–æ€§(30%) + ä¸¥è°¨æ€§(20%) + å¤šæ ·æ€§(10%)
```

#### 2. **å˜ä½“ç”Ÿæˆè®­ç»ƒ**
```python
# ä¸ºé«˜å¥–åŠ±é—®é¢˜ç”Ÿæˆè®­ç»ƒæ ·æœ¬
variation_types = [
    "context_change",        # æ”¹å˜æ•°å­¦èƒŒæ™¯ï¼Œä¿æŒè§£æ³•
    "parameter_change",      # ä¿®æ”¹å‚æ•°ï¼Œä¿æŒç»“æ„
    "practical_application"  # æ·»åŠ å®é™…åº”ç”¨åœºæ™¯
]

# åˆ›å»ºå˜ä½“è®­ç»ƒå¯¹è¯
training_example = [
    {"role": "user", "content": "Change context: [original_problem]"},
    {"role": "assistant", "content": "Variation: [generated_variation]"}
]
```

#### 3. **åè°ƒè®­ç»ƒ**
```python
# åˆå¹¶åŸå§‹å’Œå˜ä½“è®­ç»ƒæ•°æ®
all_conversations = original_conversations + variation_conversations

# è”åˆè®­ç»ƒï¼ˆæ›´ä½å­¦ä¹ ç‡ç²¾è°ƒï¼‰
trainer = SFTTrainer(
    train_dataset=step_dataset,
    args=SFTConfig(
        learning_rate=1e-5,  # æ›´ä½çš„å­¦ä¹ ç‡
        max_steps=3,         # çŸ­æ­¥è®­ç»ƒ
        fp16=False, bf16=True
    )
)
```

### å¥–åŠ±è®¡ç®—ç³»ç»Ÿ
```python
class RewardCalculator:
    def calculate_reward(self, question, history, group):
        difficulty = self.calculate_difficulty(question)      # å…³é”®è¯+å¤æ‚åº¦
        novelty = self.calculate_novelty(question, history)   # TF-IDFç›¸ä¼¼åº¦
        rigor = self.calculate_rigor(question)               # é€»è¾‘ä¸¥è°¨æ€§
        diversity = self.calculate_diversity(question, group) # ç»„å†…å¤šæ ·æ€§
        
        # åŠ æƒåˆå¹¶ (0.4, 0.3, 0.2, 0.1)
        return weighted_combination(difficulty, novelty, rigor, diversity)
```

## ğŸ¤– **é˜¶æ®µä¸‰ï¼šçŸ¥è¯†è’¸é¦ (Stage 3: DeepSeek-R1 Knowledge Distillation)**

### ç›®æ ‡
åˆ©ç”¨DeepSeek-R1ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œè¿›è¡ŒçŸ¥è¯†è’¸é¦ä¼˜åŒ–

### æ•™å¸ˆæ¨¡å‹é›†æˆ
```python
class DeepSeekTeacher:
    def __init__(self):
        self.client = OpenAI(
            api_key="sk-your-api-key",
            base_url="https://api.deepseek.com"
        )
    
    def evaluate_problem(self, problem):
        # 5ç»´åº¦è¯„ä¼°ï¼šä¸¥è°¨æ€§ã€éš¾åº¦ã€åˆ›æ–°æ€§ã€æ¸…æ™°åº¦ã€æ•™è‚²ä»·å€¼
        
    def improve_problem(self, problem, feedback):
        # åŸºäºåé¦ˆæ”¹è¿›é—®é¢˜
        
    def generate_variations(self, original, num=3):
        # ç”Ÿæˆæ™ºèƒ½å˜ä½“
```

### è’¸é¦è®­ç»ƒæµç¨‹
```python
for step in range(30):  # 30ä¸ªè’¸é¦æ­¥éª¤
    # 1. å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆé—®é¢˜
    student_question = self._generate_single_question()
    
    # 2. æ•™å¸ˆè¯„ä¼°
    teacher_eval = deepseek_teacher.evaluate_problem(student_question)
    
    # 3. æ•™å¸ˆæ”¹è¿›
    teacher_improved = deepseek_teacher.improve_problem(
        student_question, teacher_eval['raw_feedback']
    )
    
    # 4. åˆ›å»ºè®­ç»ƒå¯¹
    distillation_conversations.append([
        {"role": "user", "content": f"Improve: {student_question}"},
        {"role": "assistant", "content": teacher_improved}
    ])
    
    # 5. æ¯3æ­¥ç”Ÿæˆå˜ä½“
    if step % 3 == 0:
        variations = deepseek_teacher.generate_variations(student_question)
        # æ·»åŠ å˜ä½“è®­ç»ƒå¯¹...
```

## ğŸ§ª **æ¨ç†æµ‹è¯•ä¸éªŒè¯**

### ç»¼åˆæµ‹è¯•
```python
def inference_test(self):
    # 1. åŸå§‹é—®é¢˜ç”Ÿæˆæµ‹è¯•
    test_prompts = [
        "Generate a calculus competition problem:",
        "Create an algebra problem:",
        "Design a geometry proof:"
    ]
    
    # 2. æ€ç»´æ¨¡å¼æµ‹è¯•
    for prompt in test_prompts:
        # éæ€ç»´æ¨¡å¼ (enable_thinking=False)
        # æ€ç»´æ¨¡å¼ (enable_thinking=True, æ›´é•¿è¾“å‡º)
    
    # 3. å˜ä½“ç”Ÿæˆèƒ½åŠ›æµ‹è¯•
    for original_problem in generated_problems:
        variations = [
            "Context Change",
            "Parameter Change", 
            "Real-world Application"
        ]
        # è¯„ä¼°å˜ä½“è´¨é‡...
```

## ğŸ’¾ **æ¨¡å‹ä¿å­˜ä¸éƒ¨ç½²**

### HuggingFace Hubé›†æˆ
```python
def save_to_huggingface(self, stage_name):
    # 1. æœ¬åœ°ä¿å­˜
    model.save_pretrained("lora_model")
    
    # 2. ä¸Šä¼ åˆ°HF Hub
    model.push_to_hub(f"{username}/{model_name}-{stage}")
    
    # 3. å¤šæ ¼å¼ä¿å­˜
    model.push_to_hub_merged(repo_name, save_method="merged_16bit")
    model.push_to_hub_merged(repo_name, save_method="merged_4bit") 
    model.push_to_hub_gguf(repo_name, quantization_method=["q4_k_m", "q8_0"])
```

## ğŸ¯ **å…³é”®æŠ€æœ¯ç‰¹æ€§**

### 1. **æ–°é¢–æ€§çº¦æŸå±‚**
```python
class NoveltyConstraint(nn.Module):
    def forward(self, x, current_question):
        # è®¡ç®—ä¸å†å²é—®é¢˜çš„ç›¸ä¼¼åº¦
        # ç›¸ä¼¼åº¦ > 0.85 æ—¶æ–½åŠ æƒ©ç½š
        return x * penalty_factor if too_similar else x
```

### 2. **åè°ƒè®­ç»ƒç­–ç•¥**
- 40%åŸºç¡€ç”Ÿæˆ + 30%å˜ä½“ç”Ÿæˆ + 20%åˆ›æ–°ä¼˜åŒ–
- åŠ¨æ€å¥–åŠ±è°ƒæ•´
- å†å²é—®é¢˜æ•°æ®åº“ç®¡ç†ï¼ˆä¿æŒæœ€è¿‘1000ä¸ªé—®é¢˜ï¼‰

### 3. **å¤šç²¾åº¦å…¼å®¹**
- è§£å†³äº†torch_dtypeé‡å¤å‚æ•°é—®é¢˜
- æ­£ç¡®è®¾ç½®fp16=False, bf16=True
- æ·»åŠ trust_remote_code=Trueè§£å†³attention bias

## ğŸ“Š **è®­ç»ƒç›‘æ§ä¸è¯„ä¼°**

### å®æ—¶ç›‘æ§æŒ‡æ ‡
```python
# GRPOé˜¶æ®µç›‘æ§
print(f"ğŸ“Š Reward distribution: Mean={np.mean(rewards):.3f}")
print(f"ğŸ¯ Training samples: {len(original)} original + {len(variations)} variations")

# è’¸é¦é˜¶æ®µç›‘æ§  
print(f"ğŸ‘¨â€ğŸ« Teacher score: {teacher_eval['overall_score']:.2f}/5.0")
print(f"ğŸ“Š Difficulty: {difficulty:.1f}, Rigor: {rigor:.1f}")
```

### æœ€ç»ˆè¯„ä¼°
- é—®é¢˜ç”Ÿæˆè´¨é‡è¯„åˆ†
- å˜ä½“ç”Ÿæˆèƒ½åŠ›æµ‹è¯•
- æ•™å¸ˆæ¨¡å‹è®¤å¯åº¦ç»Ÿè®¡
- å¤šç»´åº¦èƒ½åŠ›å¹³è¡¡æ£€æŸ¥

è¿™ä¸ªè®­ç»ƒæµç¨‹ç¡®ä¿äº†æ¨¡å‹æ—¢èƒ½ç”Ÿæˆé«˜è´¨é‡çš„åŸåˆ›æ•°å­¦ç«èµ›é¢˜ç›®ï¼Œåˆå…·å¤‡æ™ºèƒ½å˜ä½“ç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡ä¸‰é˜¶æ®µé€’è¿›è®­ç»ƒè¾¾åˆ°ä¸“ä¸šæ°´å‡†ã€‚
